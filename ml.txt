Q  linear regression

import numpy as nm
import matplotlib. pyplot as mtp
x = [[1], [2], [3], [4], [5]]
y = (2, 4, 5, 4, 5)
from sklearn . model_selection import train_test_split
x_train, x_test, y_train, y_test= train_test_split (x, y, test_size= 1/3, random_state=0)
from sklearn. linear_model import LinearRegression 
regressor= LinearRegression()
regressor. fit (x_train, y_train) 
y_pred= regressor.predict (x_test) 
mtp.scatter (x_train, y_train, color="green") 
mtp.plot (x_test, y_pred, color="red") 
mtp.title("Salary vs Experience (Training Dataset)" ) 
mtp.xlabel ("Years of Experience") 
mtp.ylabel ("Salary (In Rupees)") 
mtp.show()

Q  Logistic regression

import numpy as np
import pandas as pd 
x = [[1],[2],[3], [4] , [5], [6], [7], [8]]
y = [9 , 8 , 7 , 6 , 5 , 4 , -1 , -2] 
from sklearn.model_selection import train_test_split
x_train, x_test,y_train, y_test = train_test_split(x, y, test_size=0.3, random_state = 0)
from sklearn.linear_model import LogisticRegression 
clas = LogisticRegression()
clas.fit(x_train,y_train ) 
y_pred = clas.predict (x_test) 
from sklearn . metrics import confusion_matrix 
cm = confusion_matrix(y_test,y_pred) 
print (cm)
import matplotlib.pyplot as plt 
plt.scatter (x, y) 
W= clas.coef_[0]
X = [1,2,3,4,5,6, 7, 8, 9, 10]
Y = [clas.intercept_[0]+W[0]*xi for xi in X] 
plt.plot (X, Y, color="red")
plt.show()

Q SVM

x = [[1],[2], [3], [4], [5], [6], [7] , [8], [9], [10], [11]]
y = [9,8, 7, 6, 5, 4, 3, 2, 1, 0, -1]
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x, y, random_state=0, test_size=0.3)
from sklearn. svm import SVC 
sv = SVC(kernel="linear")
sv.fit (x_train,y_train) 
y_pred = sv.predict (x_test) 
from sklearn.metrics import confusion_matrix 
cm=confusion_matrix(y_test, y_pred) 
print (cm)
c = sv.coef_[0]
sv.intercept_[0]
x1 = [x for x in range(1, 12) ]
y1 = [sv.intercept_[0] + c[0]*i for i in range(1, 12) ] 
import matplotlib.pyplot as plt 
plt.scatter(x, y)
plt.plot(x1, y1, color="red" )
plt.show()


Q hebbian learning 

import numpy as np
tt = np.array([[ -1, -1,1], [-1, 1, 1], [1, -1, 1], [1,1, 1]]) 
op = np.array([-1, 1, 1, 1]) 
w = np. array([0,0,0])
b= 0 
row = tt[0, :] 
row2 = tt[1, :] 
row3 = tt[2, :] 
row4 = tt[3, :]
X1 = np.array([row]) 
X2 = np.array([row2])
X3 = np.array([row3]) 
X4 = np.array([row4])
w = np.add(w,(X1*op[0])) 
w = np.add(w,(X2*op[1])) 
w = np.add(w,(X3*op[2])) 
w = np.add(w,(X4*op[3]))
w


Q MP Model 

from sklearn. linear_model import Perceptron
X = [[0,0 ], [0, 1], [1, 0], [1, 1]] 
y = [0, 0, 1, 0]
per = Perceptron() 
per.fit (X, y)
w = per.coef_
y1 = X[0][0]*w[0][0]+X[0][1]*w[0][1] 
y2 = X[1][0]*w[0][0]+X[1][1]*w[0][1] 
y3 = X[2][0]*w[0][0]+X[2][1]*w[0][1] 
y4 = X[3][0]*w[0][0]+X[3][1]*w[0][1]
def act(x1, x2):
    yin = x1*w[0][0]+x2*w[0][1]
    if yin >=2:
        return 1
    else:
        return 0
print("give x1 and x2 values for prediction: ")
x1 = int(input ("X1: ")) 
x2 = int(input ("X2: ")) 
a = act (x1, x2)
print ("Activation output: ",a)


Q percoptron learning

from sklearn.linear_model import Perceptron 
import numpy as np
X_train = np. array([[0, 0], [0, 1], [1, 0], [1, 1]]) 
y_train = np.array( [0, 1, 1, 1])
clf = Perceptron()
clf.fit (X_train, y_train)
X_test = np.array([ [0, 0], [0, 1]]) 
y_pred = clf.predict (X_test)
(y_pred)
 
